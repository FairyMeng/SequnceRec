Mon 08 May 2023 21:03:12 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 212
state = INFO
reproducibility = True
data_path = dataset/Amazon_Beauty
show_progress = True
save_dataset = False
save_dataloaders = False
benchmark_filename = None

Training Hyper Parameters:
checkpoint_dir = saved
epochs = 200
train_batch_size = 1024
learner = adam
learning_rate = 0.0001
eval_step = 2
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}
metrics = ['Recall', 'NDCG']
topk = [3, 5, 10, 20]
valid_metric = Recall@20
valid_metric_bigger = True
eval_batch_size = 128
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp'], 'item': ['item_id', 'title', 'sales_rank', 'price', 'brand', 'categories', 'sales_type']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id

Other Hyper Parameters: 
neg_sampling = None
multi_gpus = False
repeatable = True
n_layers = 4
n_heads = 8
hidden_size = 256
attribute_hidden_size = [64]
inner_size = 256
hidden_dropout_prob = 0.5
attn_dropout_prob = 0.3
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
selected_features = ['categories']
pooling_mode = sum
loss_type = CE
weight_sharing = not
fusion_type = gate
lamdas = [10]
attribute_predictor = linear
MODEL_TYPE = ModelType.SEQUENTIAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
device = cuda
train_neg_sample_args = {'strategy': 'none'}
eval_neg_sample_args = {'strategy': 'full', 'distribution': 'uniform'}


Mon 08 May 2023 21:03:19 INFO  Amazon_Beauty
The number of users: 22364
Average actions of users: 8.876358270357287
The number of items: 12102
Average actions of items: 16.403768283612923
The number of inters: 198502
The sparsity of the dataset: 99.92665707018277%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp', 'title', 'sales_type', 'sales_rank', 'categories', 'price', 'brand']
Mon 08 May 2023 21:03:21 INFO  [Training]: train_batch_size = [1024] negative sampling: [None]
Mon 08 May 2023 21:03:21 INFO  [Evaluation]: eval_batch_size = [128] eval_args: [{'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}]
Mon 08 May 2023 21:03:23 INFO  SASRecD(
  (item_embedding): Embedding(12102, 256, padding_idx=0)
  (position_embedding): Embedding(50, 256)
  (feature_embed_layer_list): ModuleList(
    (0): FeatureSeqEmbLayer()
  )
  (trm_encoder): DIFTransformerEncoder(
    (layer): ModuleList(
      (0): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (2): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (3): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (ap): ModuleList(
    (0): Linear(in_features=256, out_features=355, bias=True)
  )
  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (loss_fct): CrossEntropyLoss()
  (attribute_loss_fct): BCEWithLogitsLoss()
)
Trainable parameters: 5355783
Mon 08 May 2023 21:04:20 INFO  epoch 0 training [time: 57.26s, train loss: 1604.5222]
Mon 08 May 2023 21:05:17 INFO  epoch 1 training [time: 56.86s, train loss: 1239.8422]
Mon 08 May 2023 21:05:21 INFO  epoch 1 evaluating [time: 3.98s, valid_score: 0.026500]
Mon 08 May 2023 21:05:21 INFO  valid result: 
recall@3 : 0.006    recall@5 : 0.0098    recall@10 : 0.0174    recall@20 : 0.0265    ndcg@3 : 0.0042    ndcg@5 : 0.0058    ndcg@10 : 0.0083    ndcg@20 : 0.0105    
Mon 08 May 2023 21:05:23 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:06:19 INFO  epoch 2 training [time: 56.90s, train loss: 1214.8235]
Mon 08 May 2023 21:07:17 INFO  epoch 3 training [time: 57.27s, train loss: 1208.3723]
Mon 08 May 2023 21:07:21 INFO  epoch 3 evaluating [time: 3.97s, valid_score: 0.023500]
Mon 08 May 2023 21:07:21 INFO  valid result: 
recall@3 : 0.0061    recall@5 : 0.0094    recall@10 : 0.0152    recall@20 : 0.0235    ndcg@3 : 0.0044    ndcg@5 : 0.0058    ndcg@10 : 0.0077    ndcg@20 : 0.0098    
Mon 08 May 2023 21:08:17 INFO  epoch 4 training [time: 56.72s, train loss: 1203.7934]
Mon 08 May 2023 21:09:14 INFO  epoch 5 training [time: 57.03s, train loss: 1197.4070]
Mon 08 May 2023 21:09:18 INFO  epoch 5 evaluating [time: 4.01s, valid_score: 0.027800]
Mon 08 May 2023 21:09:18 INFO  valid result: 
recall@3 : 0.0065    recall@5 : 0.0107    recall@10 : 0.0169    recall@20 : 0.0278    ndcg@3 : 0.0043    ndcg@5 : 0.0061    ndcg@10 : 0.0081    ndcg@20 : 0.0108    
Mon 08 May 2023 21:09:20 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:10:17 INFO  epoch 6 training [time: 56.96s, train loss: 1185.3699]
Mon 08 May 2023 21:11:14 INFO  epoch 7 training [time: 57.17s, train loss: 1162.8134]
Mon 08 May 2023 21:11:18 INFO  epoch 7 evaluating [time: 3.99s, valid_score: 0.050400]
Mon 08 May 2023 21:11:18 INFO  valid result: 
recall@3 : 0.0122    recall@5 : 0.0191    recall@10 : 0.0324    recall@20 : 0.0504    ndcg@3 : 0.0086    ndcg@5 : 0.0114    ndcg@10 : 0.0157    ndcg@20 : 0.0202    
Mon 08 May 2023 21:11:20 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:12:17 INFO  epoch 8 training [time: 56.92s, train loss: 1138.2482]
Mon 08 May 2023 21:13:14 INFO  epoch 9 training [time: 57.10s, train loss: 1115.1685]
Mon 08 May 2023 21:13:18 INFO  epoch 9 evaluating [time: 3.98s, valid_score: 0.063500]
Mon 08 May 2023 21:13:18 INFO  valid result: 
recall@3 : 0.0156    recall@5 : 0.024    recall@10 : 0.0417    recall@20 : 0.0635    ndcg@3 : 0.0106    ndcg@5 : 0.014    ndcg@10 : 0.0197    ndcg@20 : 0.0252    
Mon 08 May 2023 21:13:20 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:14:17 INFO  epoch 10 training [time: 56.96s, train loss: 1094.7320]
Mon 08 May 2023 21:15:14 INFO  epoch 11 training [time: 57.14s, train loss: 1076.1869]
Mon 08 May 2023 21:15:18 INFO  epoch 11 evaluating [time: 4.00s, valid_score: 0.076200]
Mon 08 May 2023 21:15:18 INFO  valid result: 
recall@3 : 0.0192    recall@5 : 0.0304    recall@10 : 0.0492    recall@20 : 0.0762    ndcg@3 : 0.0127    ndcg@5 : 0.0173    ndcg@10 : 0.0234    ndcg@20 : 0.0302    
Mon 08 May 2023 21:15:20 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:16:16 INFO  epoch 12 training [time: 56.86s, train loss: 1059.9617]
Mon 08 May 2023 21:17:13 INFO  epoch 13 training [time: 56.60s, train loss: 1045.5926]
Mon 08 May 2023 21:17:17 INFO  epoch 13 evaluating [time: 3.89s, valid_score: 0.086100]
Mon 08 May 2023 21:17:17 INFO  valid result: 
recall@3 : 0.0233    recall@5 : 0.0366    recall@10 : 0.0589    recall@20 : 0.0861    ndcg@3 : 0.0156    ndcg@5 : 0.0211    ndcg@10 : 0.0283    ndcg@20 : 0.0351    
Mon 08 May 2023 21:17:19 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:18:16 INFO  epoch 14 training [time: 56.54s, train loss: 1031.9718]
Mon 08 May 2023 21:19:12 INFO  epoch 15 training [time: 56.18s, train loss: 1019.8229]
Mon 08 May 2023 21:19:16 INFO  epoch 15 evaluating [time: 3.90s, valid_score: 0.099100]
Mon 08 May 2023 21:19:16 INFO  valid result: 
recall@3 : 0.028    recall@5 : 0.0431    recall@10 : 0.0673    recall@20 : 0.0991    ndcg@3 : 0.0186    ndcg@5 : 0.0248    ndcg@10 : 0.0325    ndcg@20 : 0.0405    
Mon 08 May 2023 21:19:18 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:20:14 INFO  epoch 16 training [time: 56.47s, train loss: 1008.2992]
Mon 08 May 2023 21:21:10 INFO  epoch 17 training [time: 56.29s, train loss: 997.4839]
Mon 08 May 2023 21:21:14 INFO  epoch 17 evaluating [time: 3.81s, valid_score: 0.109600]
Mon 08 May 2023 21:21:14 INFO  valid result: 
recall@3 : 0.0313    recall@5 : 0.0489    recall@10 : 0.0762    recall@20 : 0.1096    ndcg@3 : 0.0204    ndcg@5 : 0.0276    ndcg@10 : 0.0365    ndcg@20 : 0.0449    
Mon 08 May 2023 21:21:16 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:22:12 INFO  epoch 18 training [time: 56.43s, train loss: 987.1248]
Mon 08 May 2023 21:23:09 INFO  epoch 19 training [time: 56.43s, train loss: 977.6138]
Mon 08 May 2023 21:23:13 INFO  epoch 19 evaluating [time: 3.89s, valid_score: 0.117400]
Mon 08 May 2023 21:23:13 INFO  valid result: 
recall@3 : 0.0364    recall@5 : 0.0545    recall@10 : 0.0823    recall@20 : 0.1174    ndcg@3 : 0.0238    ndcg@5 : 0.0312    ndcg@10 : 0.0401    ndcg@20 : 0.049    
Mon 08 May 2023 21:23:14 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:24:11 INFO  epoch 20 training [time: 56.50s, train loss: 968.3322]
Mon 08 May 2023 21:25:07 INFO  epoch 21 training [time: 56.26s, train loss: 959.4013]
Mon 08 May 2023 21:25:11 INFO  epoch 21 evaluating [time: 3.84s, valid_score: 0.125200]
Mon 08 May 2023 21:25:11 INFO  valid result: 
recall@3 : 0.0382    recall@5 : 0.0584    recall@10 : 0.0885    recall@20 : 0.1252    ndcg@3 : 0.0252    ndcg@5 : 0.0335    ndcg@10 : 0.0432    ndcg@20 : 0.0525    
Mon 08 May 2023 21:25:13 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:26:09 INFO  epoch 22 training [time: 56.65s, train loss: 950.8640]
Mon 08 May 2023 21:27:06 INFO  epoch 23 training [time: 56.42s, train loss: 942.9263]
Mon 08 May 2023 21:27:10 INFO  epoch 23 evaluating [time: 3.85s, valid_score: 0.131900]
Mon 08 May 2023 21:27:10 INFO  valid result: 
recall@3 : 0.0406    recall@5 : 0.0621    recall@10 : 0.0941    recall@20 : 0.1319    ndcg@3 : 0.0265    ndcg@5 : 0.0353    ndcg@10 : 0.0456    ndcg@20 : 0.0551    
Mon 08 May 2023 21:27:12 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:28:08 INFO  epoch 24 training [time: 56.48s, train loss: 934.6999]
Mon 08 May 2023 21:29:04 INFO  epoch 25 training [time: 56.34s, train loss: 927.2043]
Mon 08 May 2023 21:29:08 INFO  epoch 25 evaluating [time: 3.80s, valid_score: 0.135900]
Mon 08 May 2023 21:29:08 INFO  valid result: 
recall@3 : 0.0424    recall@5 : 0.0654    recall@10 : 0.0985    recall@20 : 0.1359    ndcg@3 : 0.0276    ndcg@5 : 0.037    ndcg@10 : 0.0476    ndcg@20 : 0.0571    
Mon 08 May 2023 21:29:10 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:30:06 INFO  epoch 26 training [time: 56.31s, train loss: 919.4264]
Mon 08 May 2023 21:31:03 INFO  epoch 27 training [time: 56.24s, train loss: 912.7406]
Mon 08 May 2023 21:31:06 INFO  epoch 27 evaluating [time: 3.75s, valid_score: 0.139700]
Mon 08 May 2023 21:31:06 INFO  valid result: 
recall@3 : 0.0444    recall@5 : 0.0675    recall@10 : 0.102    recall@20 : 0.1397    ndcg@3 : 0.0286    ndcg@5 : 0.038    ndcg@10 : 0.0491    ndcg@20 : 0.0586    
Mon 08 May 2023 21:31:08 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:32:05 INFO  epoch 28 training [time: 56.38s, train loss: 905.8107]
Mon 08 May 2023 21:33:02 INFO  epoch 29 training [time: 57.10s, train loss: 899.6062]
Mon 08 May 2023 21:33:06 INFO  epoch 29 evaluating [time: 3.84s, valid_score: 0.142400]
Mon 08 May 2023 21:33:06 INFO  valid result: 
recall@3 : 0.0454    recall@5 : 0.0702    recall@10 : 0.1026    recall@20 : 0.1424    ndcg@3 : 0.0291    ndcg@5 : 0.0393    ndcg@10 : 0.0497    ndcg@20 : 0.0597    
Mon 08 May 2023 21:33:08 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:34:05 INFO  epoch 30 training [time: 56.78s, train loss: 893.0980]
Mon 08 May 2023 21:35:01 INFO  epoch 31 training [time: 56.45s, train loss: 886.8589]
Mon 08 May 2023 21:35:05 INFO  epoch 31 evaluating [time: 3.76s, valid_score: 0.144700]
Mon 08 May 2023 21:35:05 INFO  valid result: 
recall@3 : 0.0464    recall@5 : 0.0712    recall@10 : 0.1051    recall@20 : 0.1447    ndcg@3 : 0.0297    ndcg@5 : 0.0398    ndcg@10 : 0.0507    ndcg@20 : 0.0607    
Mon 08 May 2023 21:35:06 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:36:03 INFO  epoch 32 training [time: 56.53s, train loss: 880.8944]
Mon 08 May 2023 21:37:00 INFO  epoch 33 training [time: 56.53s, train loss: 875.4924]
Mon 08 May 2023 21:37:03 INFO  epoch 33 evaluating [time: 3.90s, valid_score: 0.147900]
Mon 08 May 2023 21:37:03 INFO  valid result: 
recall@3 : 0.0471    recall@5 : 0.0725    recall@10 : 0.1069    recall@20 : 0.1479    ndcg@3 : 0.0301    ndcg@5 : 0.0405    ndcg@10 : 0.0515    ndcg@20 : 0.0618    
Mon 08 May 2023 21:37:06 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:38:02 INFO  epoch 34 training [time: 56.66s, train loss: 869.7305]
Mon 08 May 2023 21:38:59 INFO  epoch 35 training [time: 56.80s, train loss: 864.5761]
Mon 08 May 2023 21:39:03 INFO  epoch 35 evaluating [time: 3.80s, valid_score: 0.149200]
Mon 08 May 2023 21:39:03 INFO  valid result: 
recall@3 : 0.0477    recall@5 : 0.0738    recall@10 : 0.1077    recall@20 : 0.1492    ndcg@3 : 0.0303    ndcg@5 : 0.041    ndcg@10 : 0.0519    ndcg@20 : 0.0623    
Mon 08 May 2023 21:39:04 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:40:01 INFO  epoch 36 training [time: 56.31s, train loss: 859.7176]
Mon 08 May 2023 21:40:57 INFO  epoch 37 training [time: 56.29s, train loss: 854.4874]
Mon 08 May 2023 21:41:01 INFO  epoch 37 evaluating [time: 3.78s, valid_score: 0.150700]
Mon 08 May 2023 21:41:01 INFO  valid result: 
recall@3 : 0.0482    recall@5 : 0.0741    recall@10 : 0.1088    recall@20 : 0.1507    ndcg@3 : 0.0308    ndcg@5 : 0.0414    ndcg@10 : 0.0526    ndcg@20 : 0.0631    
Mon 08 May 2023 21:41:03 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:41:59 INFO  epoch 38 training [time: 56.20s, train loss: 849.9415]
Mon 08 May 2023 21:42:55 INFO  epoch 39 training [time: 56.15s, train loss: 845.5901]
Mon 08 May 2023 21:42:59 INFO  epoch 39 evaluating [time: 3.81s, valid_score: 0.151100]
Mon 08 May 2023 21:42:59 INFO  valid result: 
recall@3 : 0.0482    recall@5 : 0.0745    recall@10 : 0.1101    recall@20 : 0.1511    ndcg@3 : 0.0307    ndcg@5 : 0.0416    ndcg@10 : 0.0531    ndcg@20 : 0.0634    
Mon 08 May 2023 21:43:00 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:43:56 INFO  epoch 40 training [time: 56.06s, train loss: 840.3356]
Mon 08 May 2023 21:44:53 INFO  epoch 41 training [time: 56.34s, train loss: 836.3668]
Mon 08 May 2023 21:44:56 INFO  epoch 41 evaluating [time: 3.76s, valid_score: 0.151700]
Mon 08 May 2023 21:44:56 INFO  valid result: 
recall@3 : 0.049    recall@5 : 0.0749    recall@10 : 0.1113    recall@20 : 0.1517    ndcg@3 : 0.0312    ndcg@5 : 0.0418    ndcg@10 : 0.0536    ndcg@20 : 0.0637    
Mon 08 May 2023 21:44:58 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:45:55 INFO  epoch 42 training [time: 56.52s, train loss: 832.1812]
Mon 08 May 2023 21:46:51 INFO  epoch 43 training [time: 56.77s, train loss: 828.1708]
Mon 08 May 2023 21:46:55 INFO  epoch 43 evaluating [time: 3.89s, valid_score: 0.153300]
Mon 08 May 2023 21:46:55 INFO  valid result: 
recall@3 : 0.0491    recall@5 : 0.0745    recall@10 : 0.1116    recall@20 : 0.1533    ndcg@3 : 0.0318    ndcg@5 : 0.0423    ndcg@10 : 0.0543    ndcg@20 : 0.0648    
Mon 08 May 2023 21:46:57 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:47:54 INFO  epoch 44 training [time: 56.38s, train loss: 824.1608]
Mon 08 May 2023 21:48:50 INFO  epoch 45 training [time: 56.28s, train loss: 820.6945]
Mon 08 May 2023 21:48:54 INFO  epoch 45 evaluating [time: 3.72s, valid_score: 0.154000]
Mon 08 May 2023 21:48:54 INFO  valid result: 
recall@3 : 0.0488    recall@5 : 0.0752    recall@10 : 0.1119    recall@20 : 0.154    ndcg@3 : 0.0314    ndcg@5 : 0.0423    ndcg@10 : 0.0542    ndcg@20 : 0.0648    
Mon 08 May 2023 21:48:56 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:49:52 INFO  epoch 46 training [time: 56.54s, train loss: 816.7398]
Mon 08 May 2023 21:50:49 INFO  epoch 47 training [time: 56.49s, train loss: 813.2021]
Mon 08 May 2023 21:50:52 INFO  epoch 47 evaluating [time: 3.71s, valid_score: 0.155800]
Mon 08 May 2023 21:50:52 INFO  valid result: 
recall@3 : 0.0493    recall@5 : 0.0767    recall@10 : 0.1119    recall@20 : 0.1558    ndcg@3 : 0.0316    ndcg@5 : 0.0429    ndcg@10 : 0.0543    ndcg@20 : 0.0653    
Mon 08 May 2023 21:50:54 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:51:51 INFO  epoch 48 training [time: 56.43s, train loss: 809.8755]
Mon 08 May 2023 21:52:47 INFO  epoch 49 training [time: 56.42s, train loss: 806.6739]
Mon 08 May 2023 21:52:51 INFO  epoch 49 evaluating [time: 3.72s, valid_score: 0.156000]
Mon 08 May 2023 21:52:51 INFO  valid result: 
recall@3 : 0.0495    recall@5 : 0.0761    recall@10 : 0.1134    recall@20 : 0.156    ndcg@3 : 0.0322    ndcg@5 : 0.0431    ndcg@10 : 0.0552    ndcg@20 : 0.0658    
Mon 08 May 2023 21:52:53 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:53:49 INFO  epoch 50 training [time: 56.50s, train loss: 802.7763]
Mon 08 May 2023 21:54:46 INFO  epoch 51 training [time: 56.61s, train loss: 799.9143]
Mon 08 May 2023 21:54:50 INFO  epoch 51 evaluating [time: 3.74s, valid_score: 0.155800]
Mon 08 May 2023 21:54:50 INFO  valid result: 
recall@3 : 0.0502    recall@5 : 0.0771    recall@10 : 0.1138    recall@20 : 0.1558    ndcg@3 : 0.0326    ndcg@5 : 0.0438    ndcg@10 : 0.0556    ndcg@20 : 0.0661    
Mon 08 May 2023 21:55:46 INFO  epoch 52 training [time: 56.46s, train loss: 796.5829]
Mon 08 May 2023 21:56:43 INFO  epoch 53 training [time: 56.54s, train loss: 793.6180]
Mon 08 May 2023 21:56:46 INFO  epoch 53 evaluating [time: 3.70s, valid_score: 0.155500]
Mon 08 May 2023 21:56:46 INFO  valid result: 
recall@3 : 0.0509    recall@5 : 0.0772    recall@10 : 0.1129    recall@20 : 0.1555    ndcg@3 : 0.0327    ndcg@5 : 0.0435    ndcg@10 : 0.055    ndcg@20 : 0.0657    
Mon 08 May 2023 21:57:43 INFO  epoch 54 training [time: 56.42s, train loss: 790.6045]
Mon 08 May 2023 21:58:39 INFO  epoch 55 training [time: 56.59s, train loss: 787.6975]
Mon 08 May 2023 21:58:43 INFO  epoch 55 evaluating [time: 3.72s, valid_score: 0.156900]
Mon 08 May 2023 21:58:43 INFO  valid result: 
recall@3 : 0.0504    recall@5 : 0.0772    recall@10 : 0.1131    recall@20 : 0.1569    ndcg@3 : 0.0327    ndcg@5 : 0.0437    ndcg@10 : 0.0553    ndcg@20 : 0.0663    
Mon 08 May 2023 21:58:45 INFO  Saving current best: saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 21:59:42 INFO  epoch 56 training [time: 56.57s, train loss: 784.4906]
Mon 08 May 2023 22:00:38 INFO  epoch 57 training [time: 56.69s, train loss: 781.9726]
Mon 08 May 2023 22:00:42 INFO  epoch 57 evaluating [time: 3.90s, valid_score: 0.156800]
Mon 08 May 2023 22:00:42 INFO  valid result: 
recall@3 : 0.0513    recall@5 : 0.0777    recall@10 : 0.1136    recall@20 : 0.1568    ndcg@3 : 0.0334    ndcg@5 : 0.0442    ndcg@10 : 0.0558    ndcg@20 : 0.0666    
Mon 08 May 2023 22:01:39 INFO  epoch 58 training [time: 56.46s, train loss: 778.5881]
Mon 08 May 2023 22:02:35 INFO  epoch 59 training [time: 56.63s, train loss: 776.0234]
Mon 08 May 2023 22:02:39 INFO  epoch 59 evaluating [time: 3.70s, valid_score: 0.156300]
Mon 08 May 2023 22:02:39 INFO  valid result: 
recall@3 : 0.0513    recall@5 : 0.0771    recall@10 : 0.1128    recall@20 : 0.1563    ndcg@3 : 0.0334    ndcg@5 : 0.0441    ndcg@10 : 0.0556    ndcg@20 : 0.0666    
Mon 08 May 2023 22:03:35 INFO  epoch 60 training [time: 56.43s, train loss: 773.2358]
Mon 08 May 2023 22:04:32 INFO  epoch 61 training [time: 56.56s, train loss: 770.9881]
Mon 08 May 2023 22:04:36 INFO  epoch 61 evaluating [time: 3.88s, valid_score: 0.155900]
Mon 08 May 2023 22:04:36 INFO  valid result: 
recall@3 : 0.0518    recall@5 : 0.0781    recall@10 : 0.1115    recall@20 : 0.1559    ndcg@3 : 0.0335    ndcg@5 : 0.0443    ndcg@10 : 0.0551    ndcg@20 : 0.0663    
Mon 08 May 2023 22:05:32 INFO  epoch 62 training [time: 56.48s, train loss: 768.1372]
Mon 08 May 2023 22:06:29 INFO  epoch 63 training [time: 56.58s, train loss: 765.1242]
Mon 08 May 2023 22:06:33 INFO  epoch 63 evaluating [time: 3.80s, valid_score: 0.156200]
Mon 08 May 2023 22:06:33 INFO  valid result: 
recall@3 : 0.052    recall@5 : 0.0778    recall@10 : 0.1126    recall@20 : 0.1562    ndcg@3 : 0.0339    ndcg@5 : 0.0445    ndcg@10 : 0.0557    ndcg@20 : 0.0667    
Mon 08 May 2023 22:07:29 INFO  epoch 64 training [time: 56.36s, train loss: 762.8506]
Mon 08 May 2023 22:08:26 INFO  epoch 65 training [time: 56.68s, train loss: 759.6691]
Mon 08 May 2023 22:08:30 INFO  epoch 65 evaluating [time: 3.90s, valid_score: 0.156300]
Mon 08 May 2023 22:08:30 INFO  valid result: 
recall@3 : 0.0509    recall@5 : 0.0771    recall@10 : 0.1129    recall@20 : 0.1563    ndcg@3 : 0.0335    ndcg@5 : 0.0443    ndcg@10 : 0.0559    ndcg@20 : 0.0668    
Mon 08 May 2023 22:09:26 INFO  epoch 66 training [time: 56.30s, train loss: 757.2970]
Mon 08 May 2023 22:10:23 INFO  epoch 67 training [time: 56.62s, train loss: 755.3998]
Mon 08 May 2023 22:10:26 INFO  epoch 67 evaluating [time: 3.84s, valid_score: 0.154900]
Mon 08 May 2023 22:10:26 INFO  valid result: 
recall@3 : 0.0511    recall@5 : 0.0775    recall@10 : 0.1124    recall@20 : 0.1549    ndcg@3 : 0.0335    ndcg@5 : 0.0444    ndcg@10 : 0.0557    ndcg@20 : 0.0664    
Mon 08 May 2023 22:11:23 INFO  epoch 68 training [time: 56.36s, train loss: 752.0512]
Mon 08 May 2023 22:12:19 INFO  epoch 69 training [time: 56.54s, train loss: 749.9024]
Mon 08 May 2023 22:12:23 INFO  epoch 69 evaluating [time: 3.85s, valid_score: 0.155400]
Mon 08 May 2023 22:12:23 INFO  valid result: 
recall@3 : 0.0512    recall@5 : 0.0774    recall@10 : 0.1119    recall@20 : 0.1554    ndcg@3 : 0.0338    ndcg@5 : 0.0446    ndcg@10 : 0.0557    ndcg@20 : 0.0667    
Mon 08 May 2023 22:13:20 INFO  epoch 70 training [time: 56.63s, train loss: 746.8421]
Mon 08 May 2023 22:14:16 INFO  epoch 71 training [time: 56.49s, train loss: 744.3099]
Mon 08 May 2023 22:14:20 INFO  epoch 71 evaluating [time: 3.88s, valid_score: 0.155200]
Mon 08 May 2023 22:14:20 INFO  valid result: 
recall@3 : 0.0516    recall@5 : 0.0773    recall@10 : 0.1112    recall@20 : 0.1552    ndcg@3 : 0.034    ndcg@5 : 0.0446    ndcg@10 : 0.0555    ndcg@20 : 0.0666    
Mon 08 May 2023 22:15:17 INFO  epoch 72 training [time: 56.42s, train loss: 742.6945]
Mon 08 May 2023 22:16:13 INFO  epoch 73 training [time: 56.54s, train loss: 739.4416]
Mon 08 May 2023 22:16:17 INFO  epoch 73 evaluating [time: 3.82s, valid_score: 0.154700]
Mon 08 May 2023 22:16:17 INFO  valid result: 
recall@3 : 0.0512    recall@5 : 0.0779    recall@10 : 0.1111    recall@20 : 0.1547    ndcg@3 : 0.0338    ndcg@5 : 0.0448    ndcg@10 : 0.0555    ndcg@20 : 0.0665    
Mon 08 May 2023 22:17:13 INFO  epoch 74 training [time: 56.35s, train loss: 737.3975]
Mon 08 May 2023 22:18:09 INFO  epoch 75 training [time: 56.07s, train loss: 735.2943]
Mon 08 May 2023 22:18:13 INFO  epoch 75 evaluating [time: 3.84s, valid_score: 0.154300]
Mon 08 May 2023 22:18:13 INFO  valid result: 
recall@3 : 0.0512    recall@5 : 0.0782    recall@10 : 0.1113    recall@20 : 0.1543    ndcg@3 : 0.0341    ndcg@5 : 0.0452    ndcg@10 : 0.0559    ndcg@20 : 0.0667    
Mon 08 May 2023 22:19:10 INFO  epoch 76 training [time: 56.39s, train loss: 732.2434]
Mon 08 May 2023 22:20:06 INFO  epoch 77 training [time: 56.22s, train loss: 729.7147]
Mon 08 May 2023 22:20:10 INFO  epoch 77 evaluating [time: 3.80s, valid_score: 0.154600]
Mon 08 May 2023 22:20:10 INFO  valid result: 
recall@3 : 0.0516    recall@5 : 0.0769    recall@10 : 0.1099    recall@20 : 0.1546    ndcg@3 : 0.0344    ndcg@5 : 0.0449    ndcg@10 : 0.0556    ndcg@20 : 0.0668    
Mon 08 May 2023 22:20:10 INFO  Finished training, best eval result in epoch 55
Mon 08 May 2023 22:20:10 INFO  Loading model structure and parameters from saved\SASRecD-May-08-2023_21-03-23.pth
Mon 08 May 2023 22:20:14 INFO  best valid : {'recall@3': 0.0504, 'recall@5': 0.0772, 'recall@10': 0.1131, 'recall@20': 0.1569, 'ndcg@3': 0.0327, 'ndcg@5': 0.0437, 'ndcg@10': 0.0553, 'ndcg@20': 0.0663}
Mon 08 May 2023 22:20:14 INFO  test result: {'recall@3': 0.0386, 'recall@5': 0.058, 'recall@10': 0.0896, 'recall@20': 0.1264, 'ndcg@3': 0.0257, 'ndcg@5': 0.0337, 'ndcg@10': 0.0439, 'ndcg@20': 0.0531}
