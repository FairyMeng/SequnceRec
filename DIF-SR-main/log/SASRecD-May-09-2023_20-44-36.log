Tue 09 May 2023 20:44:36 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 212
state = INFO
reproducibility = True
data_path = dataset/yelp
show_progress = True
save_dataset = False
save_dataloaders = False
benchmark_filename = None

Training Hyper Parameters:
checkpoint_dir = saved
epochs = 200
train_batch_size = 1024
learner = adam
learning_rate = 0.0001
eval_step = 2
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}
metrics = ['Recall', 'NDCG']
topk = [3, 5, 10, 20]
valid_metric = Recall@20
valid_metric_bigger = True
eval_batch_size = 256
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = business_id
RATING_FIELD = stars
TIME_FIELD = date
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'date'], 'item': ['business_id', 'item_name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'item_stars', 'item_review_count', 'is_open', 'categories']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = {'date': '[1546264800,1577714400]'}
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id

Other Hyper Parameters: 
neg_sampling = None
multi_gpus = False
repeatable = True
n_layers = 4
n_heads = 8
hidden_size = 256
attribute_hidden_size = [64]
inner_size = 256
hidden_dropout_prob = 0.5
attn_dropout_prob = 0.3
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
selected_features = ['categories']
pooling_mode = sum
loss_type = CE
weight_sharing = not
fusion_type = gate
lamdas = [10]
attribute_predictor = linear
MODEL_TYPE = ModelType.SEQUENTIAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
device = cuda
train_neg_sample_args = {'strategy': 'none'}
eval_neg_sample_args = {'strategy': 'full', 'distribution': 'uniform'}


Tue 09 May 2023 20:44:51 INFO  yelp
The number of users: 30500
Average actions of users: 10.399750811502017
The number of items: 20069
Average actions of items: 15.805361769982062
The number of inters: 317182
The sparsity of the dataset: 99.94818172387231%
Remain Fields: ['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'date', 'item_name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'item_stars', 'item_review_count', 'is_open', 'categories']
Tue 09 May 2023 20:44:58 INFO  [Training]: train_batch_size = [1024] negative sampling: [None]
Tue 09 May 2023 20:44:58 INFO  [Evaluation]: eval_batch_size = [256] eval_args: [{'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}]
Tue 09 May 2023 20:45:00 INFO  SASRecD(
  (item_embedding): Embedding(20069, 256, padding_idx=0)
  (position_embedding): Embedding(50, 256)
  (feature_embed_layer_list): ModuleList(
    (0): FeatureSeqEmbLayer()
  )
  (trm_encoder): DIFTransformerEncoder(
    (layer): ModuleList(
      (0): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (2): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (3): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (ap): ModuleList(
    (0): Linear(in_features=256, out_features=1603, bias=True)
  )
  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (loss_fct): CrossEntropyLoss()
  (attribute_loss_fct): BCEWithLogitsLoss()
)
Trainable parameters: 7716071
Tue 09 May 2023 20:46:45 INFO  epoch 0 training [time: 104.67s, train loss: 2615.7562]
Tue 09 May 2023 20:48:28 INFO  epoch 1 training [time: 102.89s, train loss: 2166.6316]
Tue 09 May 2023 20:48:33 INFO  epoch 1 evaluating [time: 5.12s, valid_score: 0.017500]
Tue 09 May 2023 20:48:33 INFO  valid result: 
recall@3 : 0.0041    recall@5 : 0.0059    recall@10 : 0.0103    recall@20 : 0.0175    ndcg@3 : 0.0032    ndcg@5 : 0.004    ndcg@10 : 0.0054    ndcg@20 : 0.0072    
Tue 09 May 2023 20:48:41 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 20:50:24 INFO  epoch 2 training [time: 102.61s, train loss: 2147.3908]
Tue 09 May 2023 20:52:07 INFO  epoch 3 training [time: 103.51s, train loss: 2137.7786]
Tue 09 May 2023 20:52:12 INFO  epoch 3 evaluating [time: 5.11s, valid_score: 0.029100]
Tue 09 May 2023 20:52:12 INFO  valid result: 
recall@3 : 0.0112    recall@5 : 0.0145    recall@10 : 0.0205    recall@20 : 0.0291    ndcg@3 : 0.0088    ndcg@5 : 0.0102    ndcg@10 : 0.0121    ndcg@20 : 0.0143    
Tue 09 May 2023 20:52:21 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 20:54:04 INFO  epoch 4 training [time: 103.06s, train loss: 2126.3711]
Tue 09 May 2023 20:55:48 INFO  epoch 5 training [time: 103.43s, train loss: 2105.3415]
Tue 09 May 2023 20:55:53 INFO  epoch 5 evaluating [time: 5.11s, valid_score: 0.039300]
Tue 09 May 2023 20:55:53 INFO  valid result: 
recall@3 : 0.016    recall@5 : 0.0199    recall@10 : 0.0273    recall@20 : 0.0393    ndcg@3 : 0.0137    ndcg@5 : 0.0153    ndcg@10 : 0.0176    ndcg@20 : 0.0206    
Tue 09 May 2023 20:56:01 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 20:57:44 INFO  epoch 6 training [time: 103.08s, train loss: 2057.2704]
Tue 09 May 2023 20:59:27 INFO  epoch 7 training [time: 103.26s, train loss: 2000.6873]
Tue 09 May 2023 20:59:32 INFO  epoch 7 evaluating [time: 5.16s, valid_score: 0.050200]
Tue 09 May 2023 20:59:32 INFO  valid result: 
recall@3 : 0.016    recall@5 : 0.0214    recall@10 : 0.0322    recall@20 : 0.0502    ndcg@3 : 0.0126    ndcg@5 : 0.0147    ndcg@10 : 0.0183    ndcg@20 : 0.0228    
Tue 09 May 2023 20:59:41 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:01:24 INFO  epoch 8 training [time: 103.00s, train loss: 1940.6483]
Tue 09 May 2023 21:03:07 INFO  epoch 9 training [time: 103.51s, train loss: 1888.8545]
Tue 09 May 2023 21:03:12 INFO  epoch 9 evaluating [time: 5.15s, valid_score: 0.061400]
Tue 09 May 2023 21:03:12 INFO  valid result: 
recall@3 : 0.0193    recall@5 : 0.0262    recall@10 : 0.0407    recall@20 : 0.0614    ndcg@3 : 0.0155    ndcg@5 : 0.0183    ndcg@10 : 0.0229    ndcg@20 : 0.0281    
Tue 09 May 2023 21:03:20 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:05:04 INFO  epoch 10 training [time: 103.56s, train loss: 1847.0978]
Tue 09 May 2023 21:06:48 INFO  epoch 11 training [time: 103.52s, train loss: 1812.6122]
Tue 09 May 2023 21:06:53 INFO  epoch 11 evaluating [time: 5.14s, valid_score: 0.071000]
Tue 09 May 2023 21:06:53 INFO  valid result: 
recall@3 : 0.0244    recall@5 : 0.0328    recall@10 : 0.0477    recall@20 : 0.071    ndcg@3 : 0.0199    ndcg@5 : 0.0233    ndcg@10 : 0.028    ndcg@20 : 0.0339    
Tue 09 May 2023 21:07:01 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:08:44 INFO  epoch 12 training [time: 103.33s, train loss: 1782.3517]
Tue 09 May 2023 21:10:28 INFO  epoch 13 training [time: 103.53s, train loss: 1754.4520]
Tue 09 May 2023 21:10:33 INFO  epoch 13 evaluating [time: 5.10s, valid_score: 0.079500]
Tue 09 May 2023 21:10:33 INFO  valid result: 
recall@3 : 0.0275    recall@5 : 0.036    recall@10 : 0.0548    recall@20 : 0.0795    ndcg@3 : 0.0224    ndcg@5 : 0.0259    ndcg@10 : 0.0319    ndcg@20 : 0.0381    
Tue 09 May 2023 21:10:41 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:12:24 INFO  epoch 14 training [time: 103.17s, train loss: 1728.8662]
Tue 09 May 2023 21:14:08 INFO  epoch 15 training [time: 103.41s, train loss: 1705.0347]
Tue 09 May 2023 21:14:13 INFO  epoch 15 evaluating [time: 5.12s, valid_score: 0.087100]
Tue 09 May 2023 21:14:13 INFO  valid result: 
recall@3 : 0.0291    recall@5 : 0.0392    recall@10 : 0.0589    recall@20 : 0.0871    ndcg@3 : 0.0235    ndcg@5 : 0.0277    ndcg@10 : 0.0339    ndcg@20 : 0.041    
Tue 09 May 2023 21:14:21 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:16:05 INFO  epoch 16 training [time: 103.36s, train loss: 1682.5605]
Tue 09 May 2023 21:17:48 INFO  epoch 17 training [time: 103.39s, train loss: 1661.1386]
Tue 09 May 2023 21:17:53 INFO  epoch 17 evaluating [time: 5.16s, valid_score: 0.092200]
Tue 09 May 2023 21:17:53 INFO  valid result: 
recall@3 : 0.0307    recall@5 : 0.0416    recall@10 : 0.0624    recall@20 : 0.0922    ndcg@3 : 0.0249    ndcg@5 : 0.0294    ndcg@10 : 0.0361    ndcg@20 : 0.0436    
Tue 09 May 2023 21:18:02 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:19:45 INFO  epoch 18 training [time: 103.95s, train loss: 1641.0630]
Tue 09 May 2023 21:21:29 INFO  epoch 19 training [time: 103.39s, train loss: 1622.0701]
Tue 09 May 2023 21:21:34 INFO  epoch 19 evaluating [time: 5.14s, valid_score: 0.095200]
Tue 09 May 2023 21:21:34 INFO  valid result: 
recall@3 : 0.0325    recall@5 : 0.0433    recall@10 : 0.0644    recall@20 : 0.0952    ndcg@3 : 0.0262    ndcg@5 : 0.0306    ndcg@10 : 0.0374    ndcg@20 : 0.0451    
Tue 09 May 2023 21:21:42 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:23:26 INFO  epoch 20 training [time: 103.28s, train loss: 1604.4641]
Tue 09 May 2023 21:25:09 INFO  epoch 21 training [time: 103.60s, train loss: 1587.3766]
Tue 09 May 2023 21:25:14 INFO  epoch 21 evaluating [time: 5.15s, valid_score: 0.098000]
Tue 09 May 2023 21:25:14 INFO  valid result: 
recall@3 : 0.0328    recall@5 : 0.045    recall@10 : 0.0664    recall@20 : 0.098    ndcg@3 : 0.0264    ndcg@5 : 0.0314    ndcg@10 : 0.0383    ndcg@20 : 0.0462    
Tue 09 May 2023 21:25:23 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:27:06 INFO  epoch 22 training [time: 103.50s, train loss: 1571.5542]
Tue 09 May 2023 21:28:50 INFO  epoch 23 training [time: 103.52s, train loss: 1556.1556]
Tue 09 May 2023 21:28:55 INFO  epoch 23 evaluating [time: 5.14s, valid_score: 0.099900]
Tue 09 May 2023 21:28:55 INFO  valid result: 
recall@3 : 0.0338    recall@5 : 0.0452    recall@10 : 0.0682    recall@20 : 0.0999    ndcg@3 : 0.0271    ndcg@5 : 0.0318    ndcg@10 : 0.0392    ndcg@20 : 0.0471    
Tue 09 May 2023 21:29:03 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:30:47 INFO  epoch 24 training [time: 103.87s, train loss: 1542.0019]
Tue 09 May 2023 21:32:31 INFO  epoch 25 training [time: 103.75s, train loss: 1528.8687]
Tue 09 May 2023 21:32:36 INFO  epoch 25 evaluating [time: 5.17s, valid_score: 0.102000]
Tue 09 May 2023 21:32:36 INFO  valid result: 
recall@3 : 0.0343    recall@5 : 0.0456    recall@10 : 0.0688    recall@20 : 0.102    ndcg@3 : 0.0274    ndcg@5 : 0.032    ndcg@10 : 0.0395    ndcg@20 : 0.0478    
Tue 09 May 2023 21:32:44 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:34:27 INFO  epoch 26 training [time: 103.19s, train loss: 1515.1742]
Tue 09 May 2023 21:36:11 INFO  epoch 27 training [time: 103.32s, train loss: 1503.2292]
Tue 09 May 2023 21:36:16 INFO  epoch 27 evaluating [time: 5.13s, valid_score: 0.103400]
Tue 09 May 2023 21:36:16 INFO  valid result: 
recall@3 : 0.0347    recall@5 : 0.0464    recall@10 : 0.0695    recall@20 : 0.1034    ndcg@3 : 0.0278    ndcg@5 : 0.0325    ndcg@10 : 0.04    ndcg@20 : 0.0485    
Tue 09 May 2023 21:36:24 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:38:07 INFO  epoch 28 training [time: 103.22s, train loss: 1492.1972]
Tue 09 May 2023 21:39:50 INFO  epoch 29 training [time: 103.19s, train loss: 1480.7364]
Tue 09 May 2023 21:39:56 INFO  epoch 29 evaluating [time: 5.13s, valid_score: 0.105100]
Tue 09 May 2023 21:39:56 INFO  valid result: 
recall@3 : 0.0351    recall@5 : 0.0472    recall@10 : 0.0716    recall@20 : 0.1051    ndcg@3 : 0.0279    ndcg@5 : 0.0329    ndcg@10 : 0.0407    ndcg@20 : 0.0491    
Tue 09 May 2023 21:40:04 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:41:48 INFO  epoch 30 training [time: 103.54s, train loss: 1471.1259]
Tue 09 May 2023 21:43:31 INFO  epoch 31 training [time: 103.23s, train loss: 1461.8913]
Tue 09 May 2023 21:43:36 INFO  epoch 31 evaluating [time: 5.16s, valid_score: 0.105800]
Tue 09 May 2023 21:43:36 INFO  valid result: 
recall@3 : 0.0358    recall@5 : 0.0468    recall@10 : 0.0711    recall@20 : 0.1058    ndcg@3 : 0.0283    ndcg@5 : 0.0328    ndcg@10 : 0.0407    ndcg@20 : 0.0494    
Tue 09 May 2023 21:43:44 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:45:27 INFO  epoch 32 training [time: 103.36s, train loss: 1453.0677]
Tue 09 May 2023 21:47:11 INFO  epoch 33 training [time: 103.48s, train loss: 1444.7341]
Tue 09 May 2023 21:47:16 INFO  epoch 33 evaluating [time: 5.15s, valid_score: 0.106800]
Tue 09 May 2023 21:47:16 INFO  valid result: 
recall@3 : 0.0355    recall@5 : 0.0473    recall@10 : 0.0712    recall@20 : 0.1068    ndcg@3 : 0.0282    ndcg@5 : 0.033    ndcg@10 : 0.0407    ndcg@20 : 0.0496    
Tue 09 May 2023 21:47:24 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:49:08 INFO  epoch 34 training [time: 103.27s, train loss: 1436.4534]
Tue 09 May 2023 21:50:51 INFO  epoch 35 training [time: 103.70s, train loss: 1428.4758]
Tue 09 May 2023 21:50:57 INFO  epoch 35 evaluating [time: 5.13s, valid_score: 0.107100]
Tue 09 May 2023 21:50:57 INFO  valid result: 
recall@3 : 0.0361    recall@5 : 0.0476    recall@10 : 0.0724    recall@20 : 0.1071    ndcg@3 : 0.0287    ndcg@5 : 0.0334    ndcg@10 : 0.0413    ndcg@20 : 0.05    
Tue 09 May 2023 21:51:05 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 21:52:48 INFO  epoch 36 training [time: 103.30s, train loss: 1421.3617]
Tue 09 May 2023 21:54:31 INFO  epoch 37 training [time: 103.45s, train loss: 1414.8092]
Tue 09 May 2023 21:54:37 INFO  epoch 37 evaluating [time: 5.13s, valid_score: 0.107000]
Tue 09 May 2023 21:54:37 INFO  valid result: 
recall@3 : 0.0362    recall@5 : 0.048    recall@10 : 0.0722    recall@20 : 0.107    ndcg@3 : 0.0287    ndcg@5 : 0.0335    ndcg@10 : 0.0412    ndcg@20 : 0.05    
Tue 09 May 2023 21:56:20 INFO  epoch 38 training [time: 103.39s, train loss: 1407.9579]
Tue 09 May 2023 21:58:03 INFO  epoch 39 training [time: 103.20s, train loss: 1401.6878]
Tue 09 May 2023 21:58:08 INFO  epoch 39 evaluating [time: 5.12s, valid_score: 0.107900]
Tue 09 May 2023 21:58:08 INFO  valid result: 
recall@3 : 0.0365    recall@5 : 0.0484    recall@10 : 0.0724    recall@20 : 0.1079    ndcg@3 : 0.0288    ndcg@5 : 0.0337    ndcg@10 : 0.0413    ndcg@20 : 0.0503    
Tue 09 May 2023 21:58:17 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 22:00:00 INFO  epoch 40 training [time: 103.17s, train loss: 1395.6111]
Tue 09 May 2023 22:01:43 INFO  epoch 41 training [time: 103.30s, train loss: 1389.6693]
Tue 09 May 2023 22:01:48 INFO  epoch 41 evaluating [time: 5.17s, valid_score: 0.108800]
Tue 09 May 2023 22:01:48 INFO  valid result: 
recall@3 : 0.0365    recall@5 : 0.0488    recall@10 : 0.0728    recall@20 : 0.1088    ndcg@3 : 0.0289    ndcg@5 : 0.0339    ndcg@10 : 0.0416    ndcg@20 : 0.0506    
Tue 09 May 2023 22:01:56 INFO  Saving current best: saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 22:03:40 INFO  epoch 42 training [time: 103.08s, train loss: 1383.7497]
Tue 09 May 2023 22:05:23 INFO  epoch 43 training [time: 103.21s, train loss: 1378.7523]
Tue 09 May 2023 22:05:28 INFO  epoch 43 evaluating [time: 5.16s, valid_score: 0.108200]
Tue 09 May 2023 22:05:28 INFO  valid result: 
recall@3 : 0.0363    recall@5 : 0.0484    recall@10 : 0.0728    recall@20 : 0.1082    ndcg@3 : 0.0287    ndcg@5 : 0.0337    ndcg@10 : 0.0415    ndcg@20 : 0.0504    
Tue 09 May 2023 22:07:11 INFO  epoch 44 training [time: 103.28s, train loss: 1373.1693]
Tue 09 May 2023 22:08:55 INFO  epoch 45 training [time: 103.43s, train loss: 1367.5813]
Tue 09 May 2023 22:09:00 INFO  epoch 45 evaluating [time: 5.14s, valid_score: 0.107300]
Tue 09 May 2023 22:09:00 INFO  valid result: 
recall@3 : 0.0372    recall@5 : 0.0482    recall@10 : 0.0723    recall@20 : 0.1073    ndcg@3 : 0.0292    ndcg@5 : 0.0338    ndcg@10 : 0.0415    ndcg@20 : 0.0503    
Tue 09 May 2023 22:10:43 INFO  epoch 46 training [time: 103.24s, train loss: 1362.0609]
Tue 09 May 2023 22:12:26 INFO  epoch 47 training [time: 103.32s, train loss: 1358.0438]
Tue 09 May 2023 22:12:32 INFO  epoch 47 evaluating [time: 5.16s, valid_score: 0.107800]
Tue 09 May 2023 22:12:32 INFO  valid result: 
recall@3 : 0.037    recall@5 : 0.0486    recall@10 : 0.0724    recall@20 : 0.1078    ndcg@3 : 0.0291    ndcg@5 : 0.0339    ndcg@10 : 0.0415    ndcg@20 : 0.0504    
Tue 09 May 2023 22:14:15 INFO  epoch 48 training [time: 103.51s, train loss: 1352.3182]
Tue 09 May 2023 22:15:59 INFO  epoch 49 training [time: 103.54s, train loss: 1347.8656]
Tue 09 May 2023 22:16:04 INFO  epoch 49 evaluating [time: 5.15s, valid_score: 0.107600]
Tue 09 May 2023 22:16:04 INFO  valid result: 
recall@3 : 0.0364    recall@5 : 0.0488    recall@10 : 0.0715    recall@20 : 0.1076    ndcg@3 : 0.0288    ndcg@5 : 0.0339    ndcg@10 : 0.0412    ndcg@20 : 0.0503    
Tue 09 May 2023 22:17:47 INFO  epoch 50 training [time: 103.55s, train loss: 1342.1316]
Tue 09 May 2023 22:19:31 INFO  epoch 51 training [time: 103.56s, train loss: 1337.9567]
Tue 09 May 2023 22:19:36 INFO  epoch 51 evaluating [time: 5.18s, valid_score: 0.106100]
Tue 09 May 2023 22:19:36 INFO  valid result: 
recall@3 : 0.0366    recall@5 : 0.0482    recall@10 : 0.0712    recall@20 : 0.1061    ndcg@3 : 0.0289    ndcg@5 : 0.0336    ndcg@10 : 0.041    ndcg@20 : 0.0498    
Tue 09 May 2023 22:21:19 INFO  epoch 52 training [time: 103.41s, train loss: 1334.0292]
Tue 09 May 2023 22:23:03 INFO  epoch 53 training [time: 103.16s, train loss: 1329.5550]
Tue 09 May 2023 22:23:08 INFO  epoch 53 evaluating [time: 5.16s, valid_score: 0.106300]
Tue 09 May 2023 22:23:08 INFO  valid result: 
recall@3 : 0.0369    recall@5 : 0.0486    recall@10 : 0.0716    recall@20 : 0.1063    ndcg@3 : 0.0291    ndcg@5 : 0.0339    ndcg@10 : 0.0413    ndcg@20 : 0.05    
Tue 09 May 2023 22:24:51 INFO  epoch 54 training [time: 103.49s, train loss: 1324.3381]
Tue 09 May 2023 22:26:35 INFO  epoch 55 training [time: 103.59s, train loss: 1320.1024]
Tue 09 May 2023 22:26:40 INFO  epoch 55 evaluating [time: 5.15s, valid_score: 0.105100]
Tue 09 May 2023 22:26:40 INFO  valid result: 
recall@3 : 0.0369    recall@5 : 0.0487    recall@10 : 0.071    recall@20 : 0.1051    ndcg@3 : 0.0291    ndcg@5 : 0.0339    ndcg@10 : 0.0411    ndcg@20 : 0.0497    
Tue 09 May 2023 22:28:23 INFO  epoch 56 training [time: 103.14s, train loss: 1316.1218]
Tue 09 May 2023 22:30:06 INFO  epoch 57 training [time: 103.37s, train loss: 1310.9475]
Tue 09 May 2023 22:30:12 INFO  epoch 57 evaluating [time: 5.16s, valid_score: 0.104700]
Tue 09 May 2023 22:30:12 INFO  valid result: 
recall@3 : 0.0361    recall@5 : 0.0485    recall@10 : 0.0714    recall@20 : 0.1047    ndcg@3 : 0.0287    ndcg@5 : 0.0338    ndcg@10 : 0.0411    ndcg@20 : 0.0494    
Tue 09 May 2023 22:31:55 INFO  epoch 58 training [time: 103.42s, train loss: 1306.5266]
Tue 09 May 2023 22:33:39 INFO  epoch 59 training [time: 103.47s, train loss: 1302.5860]
Tue 09 May 2023 22:33:44 INFO  epoch 59 evaluating [time: 5.15s, valid_score: 0.103800]
Tue 09 May 2023 22:33:44 INFO  valid result: 
recall@3 : 0.0357    recall@5 : 0.0477    recall@10 : 0.0707    recall@20 : 0.1038    ndcg@3 : 0.0285    ndcg@5 : 0.0334    ndcg@10 : 0.0408    ndcg@20 : 0.0491    
Tue 09 May 2023 22:35:27 INFO  epoch 60 training [time: 103.14s, train loss: 1297.7334]
Tue 09 May 2023 22:37:10 INFO  epoch 61 training [time: 103.33s, train loss: 1293.6099]
Tue 09 May 2023 22:37:15 INFO  epoch 61 evaluating [time: 5.11s, valid_score: 0.103200]
Tue 09 May 2023 22:37:15 INFO  valid result: 
recall@3 : 0.0362    recall@5 : 0.0478    recall@10 : 0.0705    recall@20 : 0.1032    ndcg@3 : 0.0287    ndcg@5 : 0.0335    ndcg@10 : 0.0408    ndcg@20 : 0.049    
Tue 09 May 2023 22:38:59 INFO  epoch 62 training [time: 103.31s, train loss: 1289.1893]
Tue 09 May 2023 22:40:42 INFO  epoch 63 training [time: 103.18s, train loss: 1284.8335]
Tue 09 May 2023 22:40:47 INFO  epoch 63 evaluating [time: 5.16s, valid_score: 0.103300]
Tue 09 May 2023 22:40:47 INFO  valid result: 
recall@3 : 0.0358    recall@5 : 0.0475    recall@10 : 0.0701    recall@20 : 0.1033    ndcg@3 : 0.0286    ndcg@5 : 0.0334    ndcg@10 : 0.0406    ndcg@20 : 0.049    
Tue 09 May 2023 22:40:47 INFO  Finished training, best eval result in epoch 41
Tue 09 May 2023 22:40:48 INFO  Loading model structure and parameters from saved\SASRecD-May-09-2023_20-45-00.pth
Tue 09 May 2023 22:40:54 INFO  best valid : {'recall@3': 0.0365, 'recall@5': 0.0488, 'recall@10': 0.0728, 'recall@20': 0.1088, 'ndcg@3': 0.0289, 'ndcg@5': 0.0339, 'ndcg@10': 0.0416, 'ndcg@20': 0.0506}
Tue 09 May 2023 22:40:54 INFO  test result: {'recall@3': 0.037, 'recall@5': 0.0467, 'recall@10': 0.0675, 'recall@20': 0.1004, 'ndcg@3': 0.0304, 'ndcg@5': 0.0344, 'ndcg@10': 0.0411, 'ndcg@20': 0.0493}
