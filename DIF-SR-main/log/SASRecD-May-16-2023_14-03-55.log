Tue 16 May 2023 14:03:55 INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 212
state = INFO
reproducibility = True
data_path = dataset/Amazon_Toys_and_Games
show_progress = True
save_dataset = False
save_dataloaders = False
benchmark_filename = None

Training Hyper Parameters:
checkpoint_dir = saved
epochs = 200
train_batch_size = 2048
learner = adam
learning_rate = 0.0001
eval_step = 2
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}
metrics = ['Recall', 'NDCG']
topk = [3, 5, 10, 20]
valid_metric = Recall@20
valid_metric_bigger = True
eval_batch_size = 256
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 	
seq_separator =  
USER_ID_FIELD = user_id
ITEM_ID_FIELD = item_id
RATING_FIELD = rating
TIME_FIELD = timestamp
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp'], 'item': ['item_id', 'title', 'sales_rank', 'price', 'brand', 'categories', 'sales_type']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = True
user_inter_num_interval = [5,inf)
item_inter_num_interval = [5,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id

Other Hyper Parameters: 
neg_sampling = None
multi_gpus = False
repeatable = True
n_layers = 3
n_heads = 4
hidden_size = 256
attribute_hidden_size = [64]
inner_size = 256
hidden_dropout_prob = 0.5
attn_dropout_prob = 0.3
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
selected_features = ['categories']
pooling_mode = sum
loss_type = CE
weight_sharing = not
fusion_type = gate
lamdas = [5]
attribute_predictor = linear
MODEL_TYPE = ModelType.SEQUENTIAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
device = cuda
train_neg_sample_args = {'strategy': 'none'}
eval_neg_sample_args = {'strategy': 'full', 'distribution': 'uniform'}


Tue 16 May 2023 14:03:55 INFO  Amazon_Toys_and_Games
The number of users: 9446
Average actions of users: 8.863313922710429
The number of items: 4709
Average actions of items: 17.781223449447747
The number of inters: 83714
The sparsity of the dataset: 99.81179920134375%
Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp', 'title', 'price', 'sales_type', 'sales_rank', 'brand', 'categories']
Tue 16 May 2023 14:03:56 INFO  [Training]: train_batch_size = [2048] negative sampling: [None]
Tue 16 May 2023 14:03:56 INFO  [Evaluation]: eval_batch_size = [256] eval_args: [{'split': {'LS': 'valid_and_test'}, 'group_by': 'user', 'order': 'TO', 'mode': 'full'}]
Tue 16 May 2023 14:03:58 INFO  SASRecD(
  (item_embedding): Embedding(4709, 256, padding_idx=0)
  (position_embedding): Embedding(50, 256)
  (feature_embed_layer_list): ModuleList(
    (0): FeatureSeqEmbLayer()
  )
  (trm_encoder): DIFTransformerEncoder(
    (layer): ModuleList(
      (0): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (2): DIFTransformerLayer(
        (multi_head_attention): DIFMultiHeadAttention(
          (query): Linear(in_features=256, out_features=256, bias=True)
          (key): Linear(in_features=256, out_features=256, bias=True)
          (value): Linear(in_features=256, out_features=256, bias=True)
          (query_p): Linear(in_features=256, out_features=256, bias=True)
          (key_p): Linear(in_features=256, out_features=256, bias=True)
          (query_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (key_layers): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
          )
          (fusion_layer): VanillaAttention(
            (projection): Sequential(
              (0): Linear(in_features=50, out_features=50, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=50, out_features=1, bias=True)
            )
          )
          (attn_dropout): Dropout(p=0.3, inplace=False)
          (dense): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=256, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=256, bias=True)
          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (ap): ModuleList(
    (0): Linear(in_features=256, out_features=786, bias=True)
  )
  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (loss_fct): CrossEntropyLoss()
  (attribute_loss_fct): BCEWithLogitsLoss()
)
Trainable parameters: 3035661
Tue 16 May 2023 14:04:18 INFO  epoch 0 training [time: 19.73s, train loss: 327.5412]
Tue 16 May 2023 14:04:35 INFO  epoch 1 training [time: 17.66s, train loss: 302.7267]
Tue 16 May 2023 14:04:36 INFO  epoch 1 evaluating [time: 1.09s, valid_score: 0.082400]
Tue 16 May 2023 14:04:36 INFO  valid result: 
recall@3 : 0.0683    recall@5 : 0.0704    recall@10 : 0.0752    recall@20 : 0.0824    ndcg@3 : 0.0673    ndcg@5 : 0.0682    ndcg@10 : 0.0697    ndcg@20 : 0.0715    
Tue 16 May 2023 14:04:37 INFO  Saving current best: saved\SASRecD-May-16-2023_14-03-58.pth
Tue 16 May 2023 14:04:54 INFO  epoch 2 training [time: 17.53s, train loss: 256.1592]
Tue 16 May 2023 14:05:12 INFO  epoch 3 training [time: 17.65s, train loss: 240.5802]
Tue 16 May 2023 14:05:13 INFO  epoch 3 evaluating [time: 1.11s, valid_score: 0.082100]
Tue 16 May 2023 14:05:13 INFO  valid result: 
recall@3 : 0.0347    recall@5 : 0.0463    recall@10 : 0.0619    recall@20 : 0.0821    ndcg@3 : 0.0281    ndcg@5 : 0.0328    ndcg@10 : 0.0379    ndcg@20 : 0.0429    
Tue 16 May 2023 14:05:31 INFO  epoch 4 training [time: 17.60s, train loss: 233.5215]
Tue 16 May 2023 14:05:48 INFO  epoch 5 training [time: 17.50s, train loss: 229.4856]
Tue 16 May 2023 14:05:49 INFO  epoch 5 evaluating [time: 1.10s, valid_score: 0.087300]
Tue 16 May 2023 14:05:49 INFO  valid result: 
recall@3 : 0.052    recall@5 : 0.0598    recall@10 : 0.0704    recall@20 : 0.0873    ndcg@3 : 0.0465    ndcg@5 : 0.0497    ndcg@10 : 0.0531    ndcg@20 : 0.0574    
Tue 16 May 2023 14:05:50 INFO  Saving current best: saved\SASRecD-May-16-2023_14-03-58.pth
Tue 16 May 2023 14:06:07 INFO  epoch 6 training [time: 17.64s, train loss: 226.6517]
Tue 16 May 2023 14:06:25 INFO  epoch 7 training [time: 17.69s, train loss: 224.5953]
Tue 16 May 2023 14:06:26 INFO  epoch 7 evaluating [time: 1.09s, valid_score: 0.088000]
Tue 16 May 2023 14:06:26 INFO  valid result: 
recall@3 : 0.0632    recall@5 : 0.0669    recall@10 : 0.0755    recall@20 : 0.088    ndcg@3 : 0.0595    ndcg@5 : 0.061    ndcg@10 : 0.0638    ndcg@20 : 0.0669    
Tue 16 May 2023 14:06:26 INFO  Saving current best: saved\SASRecD-May-16-2023_14-03-58.pth
Tue 16 May 2023 14:06:44 INFO  epoch 8 training [time: 17.64s, train loss: 223.0509]
Tue 16 May 2023 14:07:02 INFO  epoch 9 training [time: 17.62s, train loss: 221.5298]
Tue 16 May 2023 14:07:03 INFO  epoch 9 evaluating [time: 1.09s, valid_score: 0.091900]
Tue 16 May 2023 14:07:03 INFO  valid result: 
recall@3 : 0.067    recall@5 : 0.0715    recall@10 : 0.0791    recall@20 : 0.0919    ndcg@3 : 0.0642    ndcg@5 : 0.066    ndcg@10 : 0.0684    ndcg@20 : 0.0716    
Tue 16 May 2023 14:07:03 INFO  Saving current best: saved\SASRecD-May-16-2023_14-03-58.pth
Tue 16 May 2023 14:07:21 INFO  epoch 10 training [time: 17.60s, train loss: 220.4083]
Tue 16 May 2023 14:07:38 INFO  epoch 11 training [time: 17.61s, train loss: 218.6450]
Tue 16 May 2023 14:07:40 INFO  epoch 11 evaluating [time: 1.09s, valid_score: 0.096200]
Tue 16 May 2023 14:07:40 INFO  valid result: 
recall@3 : 0.071    recall@5 : 0.0754    recall@10 : 0.084    recall@20 : 0.0962    ndcg@3 : 0.0677    ndcg@5 : 0.0695    ndcg@10 : 0.0722    ndcg@20 : 0.0753    
Tue 16 May 2023 14:07:40 INFO  Saving current best: saved\SASRecD-May-16-2023_14-03-58.pth
Tue 16 May 2023 14:07:58 INFO  epoch 12 training [time: 18.18s, train loss: 217.7155]
